{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les importations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import base64\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import kagglehub\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extraction du dataset Kaggle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dataset():\n",
    "    path = kagglehub.dataset_download(\"maharshipandya/-spotify-tracks-dataset\")\n",
    "    files = os.listdir(path)\n",
    "    for f in files:\n",
    "        if f.endswith(\".csv\"):\n",
    "            return pd.read_csv(os.path.join(path, f))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENT_ID = os.getenv(\"SPOTIFY_CLIENT_ID\")\n",
    "CLIENT_SECRET = os.getenv(\"SPOTIFY_CLIENT_SECRET\")\n",
    "OUTPUT_FILE = 'dataset_final.json'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cartographie des r√©gions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_region_from_code(code):\n",
    "    \"\"\"Transforme un code pays (2 lettres) en R√©gion globale.\"\"\"\n",
    "    if not code or len(code) != 2: return \"Inconnu\"\n",
    "    \n",
    "    code = code.upper()\n",
    "    \n",
    "    # Am√©rique du Nord\n",
    "    if code in ['US', 'CA', 'MX']: return \"Am√©rique du Nord\"\n",
    "    \n",
    "    # Europe \n",
    "    if code in ['GB', 'FR', 'DE', 'SE', 'IT', 'ES', 'NL', 'NO', 'DK', 'IE', 'BE', 'CH']: return \"Europe\"\n",
    "    \n",
    "    # Asie \n",
    "    if code in ['KR', 'JP', 'CN', 'IN', 'TW']: return \"Asie\"\n",
    "    \n",
    "    # Am√©rique Latine\n",
    "    if code in ['BR', 'AR', 'CO', 'PR', 'CL']: return \"Am√©rique Latine/ Am√©rique du Sud\"\n",
    "    \n",
    "    # Oc√©anie\n",
    "    if code in ['AU', 'NZ']: return \"Oc√©anie\"\n",
    "    \n",
    "    return \"Reste du Monde\" # Afrique, etc. ou codes rares\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. connexion √† spotify developpers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spotify_token():\n",
    "    auth_url = \"https://accounts.spotify.com/api/token\"\n",
    "    auth_string = f\"{CLIENT_ID}:{CLIENT_SECRET}\"\n",
    "    auth_base64 = str(base64.b64encode(auth_string.encode(\"utf-8\")), \"utf-8\")\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": \"Basic \" + auth_base64,\n",
    "        \"Content-Type\": \"application/x-www-form-urlencoded\"\n",
    "    }\n",
    "    data = {\"grant_type\": \"client_credentials\"}\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(auth_url, headers=headers, data=data, timeout=10)\n",
    "        return response.json().get(\"access_token\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur Token : {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Extraction depuis spotify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_tracks_metadata(track_ids, token):\n",
    "    ids_string = \",\".join(track_ids)\n",
    "    api_url = f\"https://api.spotify.com/v1/tracks?ids={ids_string}\"\n",
    "    headers = {\"Authorization\": \"Bearer \" + token}\n",
    "    metadata_map = {}\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(api_url, headers=headers, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            tracks_data = response.json().get('tracks', [])\n",
    "            \n",
    "            for track in tracks_data:\n",
    "                if track and 'album' in track:\n",
    "                    track_id = track['id']\n",
    "                    \n",
    "                    # 1. Ann√©e\n",
    "                    release_date = track['album']['release_date']\n",
    "                    year = release_date[:4] if release_date else None\n",
    "                    \n",
    "                    # 2. Image\n",
    "                    images = track['album']['images']\n",
    "                    image_url = images[0]['url'] if images else None\n",
    "                    \n",
    "                    # 3. Preview Audio\n",
    "                    preview_url = track.get('preview_url')\n",
    "                    \n",
    "                    # 4. PAYS & R√âGION (Via ISRC)\n",
    "                    # L'ISRC est dans external_ids (ex: \"USUM71204425\")\n",
    "                    external_ids = track.get('external_ids', {})\n",
    "                    isrc = external_ids.get('isrc', '')\n",
    "                    \n",
    "                    country_code = \"XX\"\n",
    "                    region = \"Inconnu\"\n",
    "                    \n",
    "                    if isrc and len(isrc) >= 2:\n",
    "                        country_code = isrc[:2] # Les 2 premi√®res lettres = Pays\n",
    "                        region = get_region_from_code(country_code)\n",
    "\n",
    "                    metadata_map[track_id] = {\n",
    "                        'year': year,\n",
    "                        'image': image_url,\n",
    "                        'preview': preview_url,\n",
    "                        'country': country_code,\n",
    "                        'region': region\n",
    "                    }\n",
    "                    \n",
    "        elif response.status_code == 429:\n",
    "            time.sleep(5)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Erreur Batch : {e}\")\n",
    "    \n",
    "    return metadata_map\n",
    "\n",
    "\n",
    "def format_duration(ms):\n",
    "    if pd.isna(ms): return \"0:00\"\n",
    "    seconds = int((ms / 1000) % 60)\n",
    "    minutes = int((ms / (1000 * 60)) % 60)\n",
    "    return f\"{minutes}:{seconds:02d}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Transormation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"Lecture du fichier CSV...\")\n",
    "    df = extract_dataset()\n",
    "    \n",
    "    # On garde les Hits (>30 popularit√©) pour avoir des stats pertinentes\n",
    "    df_hits = df[df['popularity'] >= 30].copy()\n",
    "    df_hits = df_hits.drop_duplicates(subset=['track_id'])\n",
    "    \n",
    "    print(f\" Traitement de {len(df_hits)} chansons...\")\n",
    "\n",
    "    token = get_spotify_token()\n",
    "    if not token: return\n",
    "\n",
    "    track_ids_list = df_hits['track_id'].tolist()\n",
    "    batch_size = 50\n",
    "    total_batches = math.ceil(len(track_ids_list) / batch_size)\n",
    "    full_metadata = {}\n",
    "\n",
    "    print(\" R√©cup√©ration des donn√©es *...\")\n",
    "\n",
    "    for i in range(total_batches):\n",
    "        start = i * batch_size\n",
    "        end = (i + 1) * batch_size\n",
    "        batch = track_ids_list[start:end]\n",
    "        \n",
    "        results = fetch_tracks_metadata(batch, token)\n",
    "        full_metadata.update(results)\n",
    "        \n",
    "        if i % 10 == 0: print(f\" Lot {i + 1}/{total_batches}...\")\n",
    "        time.sleep(0.5) # Pause API\n",
    "\n",
    "    print(\" Transformation termin√©. Fusion...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Fusion et dataset finale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def apply_enrichment(row):\n",
    "        tid = row['track_id']\n",
    "        if tid in full_metadata:\n",
    "            data = full_metadata[tid]\n",
    "            row['year'] = data['year']\n",
    "            row['image'] = data['image']\n",
    "            row['preview'] = data['preview']\n",
    "            row['country_code'] = data['country'] # Ex: US, FR, GB\n",
    "            row['region'] = data['region']        # Ex: Am√©rique du Nord, Europe\n",
    "        else:\n",
    "            row['year'] = None\n",
    "        \n",
    "        row['duration_fmt'] = format_duration(row['duration_ms'])\n",
    "        return row\n",
    "\n",
    "    df_final = df_hits.apply(apply_enrichment, axis=1)\n",
    "    df_final = df_final.dropna(subset=['year'])\n",
    "\n",
    "    # Colonnes finales\n",
    "    cols = [\n",
    "        'track_name', 'artists', 'year', 'region', 'country_code', # <-- Nouvelles colonnes\n",
    "        'image', 'preview', 'duration_fmt', 'popularity', \n",
    "        'danceability', 'energy', 'tempo', 'track_genre'\n",
    "    ]\n",
    "    \n",
    "    existing_cols = [c for c in cols if c in df_final.columns]\n",
    "    df_final = df_final[existing_cols]\n",
    "\n",
    "    df_final.to_json(OUTPUT_FILE, orient='records', indent=4)\n",
    "    print(f\" Termin√© ! Fichier '{OUTPUT_FILE}' pr√™t avec les R√©gions.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = os.getenv(\"GEMINI_API_KEY\") \n",
    "if not API_KEY:\n",
    "    raise ValueError(\"La cl√© API GEMINI_API_KEY n'est pas d√©finie.\")\n",
    "\n",
    "genai.configure(api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ROSTER = [\n",
    "    'models/gemini-1.5-flash', # Mod√®le rapide et √©conomique par d√©faut\n",
    "    'models/gemini-1.5-pro',\n",
    "    'models/gemini-1.0-pro'\n",
    "]\n",
    "\n",
    "# Noms de fichiers\n",
    "RAW_CSV = \"billboard_raw.csv\"\n",
    "ENRICHED_CSV = \"billboard_enriched_live.csv\"\n",
    "SPOTIFY_JSON = \"dataset_final.json\"\n",
    "FINAL_CSV = \"GLOBAL_HIT_1980_2023_MMSS.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_billboard_wiki(start_year, end_year):\n",
    "    base_url = \"https://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_{}\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    all_songs = []\n",
    "    print(f\"--- üåç D√©marrage du scraping de {start_year} √† {end_year} ---\")\n",
    "\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        url = base_url.format(year)\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"‚ùå Erreur HTTP {response.status_code} pour {year}\")\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            tables = soup.select('table.wikitable')\n",
    "            \n",
    "            target_table = None\n",
    "            for t in tables:\n",
    "                if \"Title\" in t.text or \"Song\" in t.text:\n",
    "                    target_table = t\n",
    "                    break\n",
    "            \n",
    "            if not target_table and tables:\n",
    "                target_table = tables[0]\n",
    "\n",
    "            if not target_table:\n",
    "                print(f\"‚ö†Ô∏è Pas de table trouv√©e pour {year}\")\n",
    "                continue\n",
    "\n",
    "            rows = target_table.find_all('tr')\n",
    "            \n",
    "            for row in rows:\n",
    "                cols = row.find_all(['th', 'td'])\n",
    "                if len(cols) < 2: continue\n",
    "                \n",
    "                track_name = \"Unknown\"\n",
    "                artist_name = \"Unknown\"\n",
    "                \n",
    "                found_title = False\n",
    "                for i, col in enumerate(cols):\n",
    "                    text = col.get_text(strip=True)\n",
    "                    if '\"' in text:\n",
    "                        track_name = text.replace('\"', '')\n",
    "                        if i + 1 < len(cols):\n",
    "                            artist_name = cols[i+1].get_text(strip=True)\n",
    "                        found_title = True\n",
    "                        break\n",
    "                \n",
    "                if not found_title and len(cols) >= 3:\n",
    "                    track_name = cols[1].get_text(strip=True).replace('\"', '')\n",
    "                    artist_name = cols[2].get_text(strip=True)\n",
    "                \n",
    "                if track_name.lower() in ['title', 'song'] or artist_name.lower() in ['artist', 'performer']:\n",
    "                    continue\n",
    "\n",
    "                if track_name != \"Unknown\":\n",
    "                    all_songs.append({\n",
    "                        'track_name': track_name,\n",
    "                        'artists': artist_name,\n",
    "                        'year': year,\n",
    "                        'region': 'US',\n",
    "                        'country_code': 'US'\n",
    "                    })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erreur sur l'ann√©e {year}: {e}\")\n",
    "\n",
    "    print(f\"‚úÖ Scraping termin√©. {len(all_songs)} chansons r√©cup√©r√©es.\")\n",
    "    return pd.DataFrame(all_songs)\n",
    "\n",
    "# Ex√©cution du scraping\n",
    "if not os.path.exists(RAW_CSV):\n",
    "    df_raw = scrape_billboard_wiki(1980, 2010) # 1980-2010 comme sp√©cifi√© dans tes fichiers\n",
    "    df_raw.to_csv(RAW_CSV, index=False)\n",
    "    print(f\"üìÅ Donn√©es brutes sauvegard√©es dans {RAW_CSV}\")\n",
    "else:\n",
    "    print(f\"üìÇ Le fichier {RAW_CSV} existe d√©j√†. Chargement...\")\n",
    "    df_raw = pd.read_csv(RAW_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_data_with_gemini(input_file, output_file, batch_size=500): # Batch size r√©duit pour √©viter les timeouts\n",
    "    if not os.path.exists(input_file):\n",
    "        print(f\"‚ùå Fichier d'entr√©e {input_file} introuvable.\")\n",
    "        return\n",
    "\n",
    "    df_raw = pd.read_csv(input_file)\n",
    "    records = df_raw.to_dict('records')\n",
    "    total_records = len(records)\n",
    "    \n",
    "    # Gestion de la reprise (si le script a plant√©)\n",
    "    start_index = 0\n",
    "    if os.path.exists(output_file):\n",
    "        try:\n",
    "            df_existing = pd.read_csv(output_file)\n",
    "            start_index = len(df_existing)\n",
    "            print(f\"üîÑ Reprise de l'enrichissement √† la ligne {start_index}...\")\n",
    "        except pd.errors.EmptyDataError:\n",
    "            # Fichier vide, on √©crit les headers\n",
    "            pd.DataFrame(columns=[\n",
    "                'track_name', 'artists', 'year', 'region', 'country_code', \n",
    "                'duration', 'tempo', 'track_genre', 'popularity'\n",
    "            ]).to_csv(output_file, index=False)\n",
    "    else:\n",
    "        # Cr√©ation du fichier\n",
    "        pd.DataFrame(columns=[\n",
    "            'track_name', 'artists', 'year', 'region', 'country_code', \n",
    "            'duration', 'tempo', 'track_genre', 'popularity'\n",
    "        ]).to_csv(output_file, index=False)\n",
    "\n",
    "    current_model_idx = 0\n",
    "    i = start_index\n",
    "\n",
    "    while i < total_records:\n",
    "        end_idx = min(i + batch_size, total_records)\n",
    "        batch = records[i : end_idx]\n",
    "        \n",
    "        if not batch: break\n",
    "        \n",
    "        batch_success = False\n",
    "        retry_count = 0\n",
    "        \n",
    "        while not batch_success:\n",
    "            if current_model_idx >= len(MODEL_ROSTER):\n",
    "                print(\"üíÄ Tous les mod√®les Gemini sont √©puis√©s. Arr√™t du script.\")\n",
    "                return\n",
    "\n",
    "            model_name = MODEL_ROSTER[current_model_idx]\n",
    "            \n",
    "            try:\n",
    "                print(f\"‚ö° Enrichissement lignes {i} √† {end_idx} ({model_name})...\")\n",
    "                model = genai.GenerativeModel(model_name)\n",
    "                \n",
    "                prompt = f\"\"\"\n",
    "                DATA: {json.dumps(batch)}\n",
    "                \n",
    "                INSTRUCTIONS:\n",
    "                Tu es un expert musical. Pour chaque chanson de la liste DATA, retourne un objet JSON avec ces cl√©s :\n",
    "                - track_name (garder l'original)\n",
    "                - artists (garder l'original)\n",
    "                - year (garder l'original)\n",
    "                - region (garder 'US')\n",
    "                - country_code (garder 'US')\n",
    "                - duration (estimation pr√©cise en SECONDES, entier)\n",
    "                - tempo (estimation BPM, entier)\n",
    "                - track_genre (genre principal unique, string court ex: 'Pop', 'Rock')\n",
    "                - popularity (score historique 0-100, entier)\n",
    "\n",
    "                Retourne UNIQUEMENT une liste JSON valide contenant exactement {len(batch)} objets. Pas de markdown.\n",
    "                \"\"\"\n",
    "                \n",
    "                response = model.generate_content(prompt)\n",
    "                text = response.text.strip()\n",
    "                if text.startswith(\"```\"): text = text.split(\"\\n\", 1)[1].rsplit(\"\\n\", 1)[0]\n",
    "                \n",
    "                try:\n",
    "                    data = json.loads(text)\n",
    "                except json.JSONDecodeError:\n",
    "                     # Fallback si le JSON est malform√© (ex: texte coup√©)\n",
    "                    print(\"‚ö†Ô∏è Erreur JSON, nouvelle tentative...\")\n",
    "                    raise ValueError(\"JSON invalide\")\n",
    "\n",
    "                if len(data) != len(batch):\n",
    "                    print(f\"‚ö†Ô∏è Mismatch: {len(data)} items re√ßus vs {len(batch)} attendus.\")\n",
    "                     # On pourrait g√©rer plus finement, mais ici on retry\n",
    "                    raise ValueError(\"Nombre d'items incorrect\")\n",
    "\n",
    "                # Sauvegarde incr√©mentale\n",
    "                df_batch = pd.DataFrame(data)\n",
    "                cols = ['track_name', 'artists', 'year', 'region', 'country_code', 'duration', 'tempo', 'track_genre', 'popularity']\n",
    "                # S√©curisation des colonnes manquantes\n",
    "                for c in cols: \n",
    "                    if c not in df_batch.columns: df_batch[c] = None\n",
    "                \n",
    "                df_batch[cols].to_csv(output_file, mode='a', header=False, index=False)\n",
    "                \n",
    "                batch_success = True\n",
    "                i += batch_size\n",
    "                time.sleep(1) # Petite pause de courtoisie\n",
    "\n",
    "            except Exception as e:\n",
    "                err_str = str(e)\n",
    "                if \"429\" in err_str or \"ResourceExhausted\" in err_str:\n",
    "                    print(f\"‚õî Quota d√©pass√© pour {model_name}. Changement de mod√®le.\")\n",
    "                    current_model_idx += 1\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è Erreur: {e}\")\n",
    "                    retry_count += 1\n",
    "                    if retry_count > 3:\n",
    "                        print(\"‚ùå Trop d'erreurs, on saute ce batch (donn√©es perdues pour ces lignes).\")\n",
    "                        i += batch_size # On avance quand m√™me pour ne pas bloquer\n",
    "                        batch_success = True\n",
    "                    else:\n",
    "                        time.sleep(2)\n",
    "\n",
    "# Lancer l'enrichissement\n",
    "enrich_data_with_gemini(RAW_CSV, ENRICHED_CSV)\n",
    "print(f\"‚úÖ Enrichissement termin√©. Donn√©es dans {ENRICHED_CSV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seconds_to_mm_ss(val):\n",
    "    \"\"\"Convertit une dur√©e en secondes (ex: 214) en format MM:SS (ex: 3:34)\"\"\"\n",
    "    try:\n",
    "        if pd.isna(val) or val == '': return None\n",
    "        seconds = int(float(val))\n",
    "        m = seconds // 60\n",
    "        s = seconds % 60\n",
    "        return f\"{m}:{s:02d}\"\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def merge_datasets():\n",
    "    print(\"--- üîÑ Fusion des datasets (Billboard Gemini + Spotify) ---\")\n",
    "\n",
    "    # 1. Chargement et pr√©paration GEMINI (1980-1999)\n",
    "    if not os.path.exists(ENRICHED_CSV):\n",
    "        print(f\"‚ùå Erreur: Fichier {ENRICHED_CSV} manquant.\")\n",
    "        return\n",
    "\n",
    "    df_old = pd.read_csv(ENRICHED_CSV)\n",
    "    \n",
    "    # Filtre temporel strict (au cas o√π Gemini a hallucin√© des ann√©es)\n",
    "    df_old = df_old[df_old['year'] < 2000].copy()\n",
    "    \n",
    "    # Conversion Duration (Secondes -> MM:SS)\n",
    "    df_old['duration'] = df_old['duration'].apply(seconds_to_mm_ss)\n",
    "    \n",
    "    # Colonnes cibles\n",
    "    cols_target = ['track_name', 'artists', 'year', 'duration', 'tempo', 'track_genre', 'popularity', 'region', 'country_code']\n",
    "    \n",
    "    # Ajout des colonnes manquantes si n√©cessaire\n",
    "    for c in cols_target:\n",
    "        if c not in df_old.columns: df_old[c] = None\n",
    "\n",
    "    print(f\"üìä Gemini (1980-1999): {len(df_old)} titres pr√™ts.\")\n",
    "\n",
    "    # 2. Chargement et pr√©paration SPOTIFY (2000-2023)\n",
    "    if not os.path.exists(SPOTIFY_JSON):\n",
    "        print(f\"‚ùå Erreur: Fichier {SPOTIFY_JSON} manquant.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        df_new = pd.read_json(SPOTIFY_JSON)\n",
    "    except ValueError:\n",
    "        print(\"‚ùå Erreur: JSON Spotify mal form√©.\")\n",
    "        return\n",
    "\n",
    "    # Conversion et filtre ann√©e\n",
    "    df_new['year'] = pd.to_numeric(df_new['year'], errors='coerce')\n",
    "    df_new = df_new[df_new['year'] >= 2000].copy()\n",
    "\n",
    "    # Mapping Duration Spotify (duration_fmt -> duration)\n",
    "    if 'duration_fmt' in df_new.columns:\n",
    "        df_new = df_new.rename(columns={'duration_fmt': 'duration'})\n",
    "    \n",
    "    # On s'assure d'avoir les m√™mes colonnes\n",
    "    for c in cols_target:\n",
    "        if c not in df_new.columns: df_new[c] = None\n",
    "    \n",
    "    df_new = df_new[cols_target]\n",
    "    print(f\"üìä Spotify (2000-2023): {len(df_new)} titres pr√™ts.\")\n",
    "\n",
    "    # 3. Concat√©nation\n",
    "    df_final = pd.concat([df_old, df_new], ignore_index=True)\n",
    "    \n",
    "    # Tri par ann√©e\n",
    "    df_final = df_final.sort_values(by='year')\n",
    "    \n",
    "    # Sauvegarde finale\n",
    "    df_final.to_csv(FINAL_CSV, index=False)\n",
    "    \n",
    "    print(f\"\\n‚úÖ FUSION R√âUSSIE : {FINAL_CSV}\")\n",
    "    print(f\"   Total lignes : {len(df_final)}\")\n",
    "    print(f\"   Aper√ßu des colonnes : {list(df_final.columns)}\")\n",
    "\n",
    "# Lancer la fusion\n",
    "merge_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seconds_to_mm_ss(val):\n",
    "    \"\"\"Convertit une dur√©e en secondes (ex: 214) en format MM:SS (ex: 3:34)\"\"\"\n",
    "    try:\n",
    "        if pd.isna(val) or val == '': return None\n",
    "        seconds = int(float(val))\n",
    "        m = seconds // 60\n",
    "        s = seconds % 60\n",
    "        return f\"{m}:{s:02d}\"\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def merge_datasets():\n",
    "    print(\"--- üîÑ Fusion des datasets (Billboard Gemini + Spotify) ---\")\n",
    "\n",
    "    # 1. Chargement et pr√©paration GEMINI (1980-1999)\n",
    "    if not os.path.exists(ENRICHED_CSV):\n",
    "        print(f\"‚ùå Erreur: Fichier {ENRICHED_CSV} manquant.\")\n",
    "        return\n",
    "\n",
    "    df_old = pd.read_csv(ENRICHED_CSV)\n",
    "    \n",
    "    # Filtre temporel strict (au cas o√π Gemini a hallucin√© des ann√©es)\n",
    "    df_old = df_old[df_old['year'] < 2000].copy()\n",
    "    \n",
    "    # Conversion Duration (Secondes -> MM:SS)\n",
    "    df_old['duration'] = df_old['duration'].apply(seconds_to_mm_ss)\n",
    "    \n",
    "    # Colonnes cibles\n",
    "    cols_target = ['track_name', 'artists', 'year', 'duration', 'tempo', 'track_genre', 'popularity', 'region', 'country_code']\n",
    "    \n",
    "    # Ajout des colonnes manquantes si n√©cessaire\n",
    "    for c in cols_target:\n",
    "        if c not in df_old.columns: df_old[c] = None\n",
    "\n",
    "    print(f\"üìä Gemini (1980-1999): {len(df_old)} titres pr√™ts.\")\n",
    "\n",
    "    # 2. Chargement et pr√©paration SPOTIFY (2000-2023)\n",
    "    if not os.path.exists(SPOTIFY_JSON):\n",
    "        print(f\"‚ùå Erreur: Fichier {SPOTIFY_JSON} manquant.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        df_new = pd.read_json(SPOTIFY_JSON)\n",
    "    except ValueError:\n",
    "        print(\"‚ùå Erreur: JSON Spotify mal form√©.\")\n",
    "        return\n",
    "\n",
    "    # Conversion et filtre ann√©e\n",
    "    df_new['year'] = pd.to_numeric(df_new['year'], errors='coerce')\n",
    "    df_new = df_new[df_new['year'] >= 2000].copy()\n",
    "\n",
    "    # Mapping Duration Spotify (duration_fmt -> duration)\n",
    "    if 'duration_fmt' in df_new.columns:\n",
    "        df_new = df_new.rename(columns={'duration_fmt': 'duration'})\n",
    "    \n",
    "    # On s'assure d'avoir les m√™mes colonnes\n",
    "    for c in cols_target:\n",
    "        if c not in df_new.columns: df_new[c] = None\n",
    "    \n",
    "    df_new = df_new[cols_target]\n",
    "    print(f\"üìä Spotify (2000-2023): {len(df_new)} titres pr√™ts.\")\n",
    "\n",
    "    # 3. Concat√©nation\n",
    "    df_final = pd.concat([df_old, df_new], ignore_index=True)\n",
    "    \n",
    "    # Tri par ann√©e\n",
    "    df_final = df_final.sort_values(by='year')\n",
    "    \n",
    "    # Sauvegarde finale\n",
    "    df_final.to_csv(FINAL_CSV, index=False)\n",
    "    \n",
    "    print(f\"\\n‚úÖ FUSION R√âUSSIE : {FINAL_CSV}\")\n",
    "    print(f\"   Total lignes : {len(df_final)}\")\n",
    "    print(f\"   Aper√ßu des colonnes : {list(df_final.columns)}\")\n",
    "\n",
    "# Lancer la fusion\n",
    "merge_datasets()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
