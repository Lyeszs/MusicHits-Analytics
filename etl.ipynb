{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les importations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import base64\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import kagglehub\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extraction du dataset Kaggle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dataset():\n",
    "    path = kagglehub.dataset_download(\"maharshipandya/-spotify-tracks-dataset\")\n",
    "    files = os.listdir(path)\n",
    "    for f in files:\n",
    "        if f.endswith(\".csv\"):\n",
    "            return pd.read_csv(os.path.join(path, f))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENT_ID = os.getenv(\"SPOTIFY_CLIENT_ID\")\n",
    "CLIENT_SECRET = os.getenv(\"SPOTIFY_CLIENT_SECRET\")\n",
    "OUTPUT_FILE = 'dataset_final.json'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cartographie des régions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_region_from_code(code):\n",
    "    \"\"\"Transforme un code pays (2 lettres) en Région globale.\"\"\"\n",
    "    if not code or len(code) != 2: return \"Inconnu\"\n",
    "    \n",
    "    code = code.upper()\n",
    "    \n",
    "    # Amérique du Nord\n",
    "    if code in ['US', 'CA', 'MX']: return \"Amérique du Nord\"\n",
    "    \n",
    "    # Europe \n",
    "    if code in ['GB', 'FR', 'DE', 'SE', 'IT', 'ES', 'NL', 'NO', 'DK', 'IE', 'BE', 'CH']: return \"Europe\"\n",
    "    \n",
    "    # Asie \n",
    "    if code in ['KR', 'JP', 'CN', 'IN', 'TW']: return \"Asie\"\n",
    "    \n",
    "    # Amérique Latine\n",
    "    if code in ['BR', 'AR', 'CO', 'PR', 'CL']: return \"Amérique Latine/ Amérique du Sud\"\n",
    "    \n",
    "    # Océanie\n",
    "    if code in ['AU', 'NZ']: return \"Océanie\"\n",
    "    \n",
    "    return \"Reste du Monde\" # Afrique, etc. ou codes rares\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. connexion à spotify developpers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spotify_token():\n",
    "    auth_url = \"https://accounts.spotify.com/api/token\"\n",
    "    auth_string = f\"{CLIENT_ID}:{CLIENT_SECRET}\"\n",
    "    auth_base64 = str(base64.b64encode(auth_string.encode(\"utf-8\")), \"utf-8\")\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": \"Basic \" + auth_base64,\n",
    "        \"Content-Type\": \"application/x-www-form-urlencoded\"\n",
    "    }\n",
    "    data = {\"grant_type\": \"client_credentials\"}\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(auth_url, headers=headers, data=data, timeout=10)\n",
    "        return response.json().get(\"access_token\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur Token : {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Extraction depuis spotify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_tracks_metadata(track_ids, token):\n",
    "    ids_string = \",\".join(track_ids)\n",
    "    api_url = f\"https://api.spotify.com/v1/tracks?ids={ids_string}\"\n",
    "    headers = {\"Authorization\": \"Bearer \" + token}\n",
    "    metadata_map = {}\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(api_url, headers=headers, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            tracks_data = response.json().get('tracks', [])\n",
    "            \n",
    "            for track in tracks_data:\n",
    "                if track and 'album' in track:\n",
    "                    track_id = track['id']\n",
    "                    \n",
    "                    # 1. Année\n",
    "                    release_date = track['album']['release_date']\n",
    "                    year = release_date[:4] if release_date else None\n",
    "                    \n",
    "                    # 2. Image\n",
    "                    images = track['album']['images']\n",
    "                    image_url = images[0]['url'] if images else None\n",
    "                    \n",
    "                    # 3. Preview Audio\n",
    "                    preview_url = track.get('preview_url')\n",
    "                    \n",
    "                    # 4. PAYS & RÉGION (Via ISRC)\n",
    "                    # L'ISRC est dans external_ids (ex: \"USUM71204425\")\n",
    "                    external_ids = track.get('external_ids', {})\n",
    "                    isrc = external_ids.get('isrc', '')\n",
    "                    \n",
    "                    country_code = \"XX\"\n",
    "                    region = \"Inconnu\"\n",
    "                    \n",
    "                    if isrc and len(isrc) >= 2:\n",
    "                        country_code = isrc[:2] # Les 2 premières lettres = Pays\n",
    "                        region = get_region_from_code(country_code)\n",
    "\n",
    "                    metadata_map[track_id] = {\n",
    "                        'year': year,\n",
    "                        'image': image_url,\n",
    "                        'preview': preview_url,\n",
    "                        'country': country_code,\n",
    "                        'region': region\n",
    "                    }\n",
    "                    \n",
    "        elif response.status_code == 429:\n",
    "            time.sleep(5)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Erreur Batch : {e}\")\n",
    "    \n",
    "    return metadata_map\n",
    "\n",
    "\n",
    "def format_duration(ms):\n",
    "    if pd.isna(ms): return \"0:00\"\n",
    "    seconds = int((ms / 1000) % 60)\n",
    "    minutes = int((ms / (1000 * 60)) % 60)\n",
    "    return f\"{minutes}:{seconds:02d}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Transormation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"Lecture du fichier CSV...\")\n",
    "    df = extract_dataset()\n",
    "    \n",
    "    # On garde les Hits (>30 popularité) pour avoir des stats pertinentes\n",
    "    df_hits = df[df['popularity'] >= 30].copy()\n",
    "    df_hits = df_hits.drop_duplicates(subset=['track_id'])\n",
    "    \n",
    "    print(f\" Traitement de {len(df_hits)} chansons...\")\n",
    "\n",
    "    token = get_spotify_token()\n",
    "    if not token: return\n",
    "\n",
    "    track_ids_list = df_hits['track_id'].tolist()\n",
    "    batch_size = 50\n",
    "    total_batches = math.ceil(len(track_ids_list) / batch_size)\n",
    "    full_metadata = {}\n",
    "\n",
    "    print(\" Récupération des données *...\")\n",
    "\n",
    "    for i in range(total_batches):\n",
    "        start = i * batch_size\n",
    "        end = (i + 1) * batch_size\n",
    "        batch = track_ids_list[start:end]\n",
    "        \n",
    "        results = fetch_tracks_metadata(batch, token)\n",
    "        full_metadata.update(results)\n",
    "        \n",
    "        if i % 10 == 0: print(f\" Lot {i + 1}/{total_batches}...\")\n",
    "        time.sleep(0.5) # Pause API\n",
    "\n",
    "    print(\" Transformation terminé. Fusion...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Fusion et dataset finale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def apply_enrichment(row):\n",
    "        tid = row['track_id']\n",
    "        if tid in full_metadata:\n",
    "            data = full_metadata[tid]\n",
    "            row['year'] = data['year']\n",
    "            row['image'] = data['image']\n",
    "            row['preview'] = data['preview']\n",
    "            row['country_code'] = data['country'] # Ex: US, FR, GB\n",
    "            row['region'] = data['region']        # Ex: Amérique du Nord, Europe\n",
    "        else:\n",
    "            row['year'] = None\n",
    "        \n",
    "        row['duration_fmt'] = format_duration(row['duration_ms'])\n",
    "        return row\n",
    "\n",
    "    df_final = df_hits.apply(apply_enrichment, axis=1)\n",
    "    df_final = df_final.dropna(subset=['year'])\n",
    "\n",
    "    # Colonnes finales\n",
    "    cols = [\n",
    "        'track_name', 'artists', 'year', 'region', 'country_code', # <-- Nouvelles colonnes\n",
    "        'image', 'preview', 'duration_fmt', 'popularity', \n",
    "        'danceability', 'energy', 'tempo', 'track_genre'\n",
    "    ]\n",
    "    \n",
    "    existing_cols = [c for c in cols if c in df_final.columns]\n",
    "    df_final = df_final[existing_cols]\n",
    "\n",
    "    df_final.to_json(OUTPUT_FILE, orient='records', indent=4)\n",
    "    print(f\" Terminé ! Fichier '{OUTPUT_FILE}' prêt avec les Régions.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
